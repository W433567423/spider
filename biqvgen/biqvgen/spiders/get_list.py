import logging,re, scrapy  # type: ignore
from biqvgen.items import GetListItem
from biqvgen.utils import console



class GetListSpider(scrapy.Spider):
    name = "get-list"  # çˆ¬è™«åç§°
    allowed_domains = ["biqugen.net"]  # å…è®¸çš„åŸŸå
    page =1 # å½“å‰é¡µæ•°
    start_urls = ["https://m.biqugen.net/full/1.html"]  # å¼€å§‹çˆ¬å–çš„url
    base_url = "https://m.biqugen.net/full/{}.html"  # ä¸‹ä¸€é¡µçš„url

    # è·å–æ€»é¡µæ•°ï¼Œå¹¶å¼€å§‹çˆ¬å–å°è¯´->list
    def parse(self, response):
        page_num = (
            int(
                response.css("div.page-book-turn::text")
                .get()
                .split("/")[1]
                .split("é¡µ")[0]
            )
        ) + 1
        for i in range(self.page, page_num):
            url = self.base_url.format(i)
            yield scrapy.Request(url, callback=self.parse_page)

    # åˆæ­¥è·å–å°è¯´idã€å°è¯´åã€å°è¯´è¿æ¥
    def parse_page(self, response):
        node_list = response.css("ul.s_m li.list-item")
        if len(node_list) == 0:
            console.log("ğŸš€ ~ node_list:", node_list)
            logging.warning(f"è¯¥é¡µæ— æ•°æ®:{response.url}")
            return
        for node in node_list:
            url = node.css("a::attr(href)").get()
            novel_id = int(url.split("book/")[-1].split("/")[0])
            novel_name = node.css("a::text").get()
            item = GetListItem(novel_id=novel_id, novel_name=novel_name)
            yield scrapy.Request(url, callback=self.parse_detail, meta={"item": item})



    # è·å–å°è¯´è¯¦ç»†ä¿¡æ¯
    def parse_detail(self, response):
        item = response.meta["item"]
        # æå–ä¿¡æ¯
        info = response.css("td.info")
        if not info:
            # å†™å…¥å¼‚å¸¸ä¿¡æ¯
            item["abnormal"] = True
            logging.error(f"{item["novel_name"]} å¼‚å¸¸:{response.url}")
            return
        item["novel_cover"] = response.css("div.bookinfo img::attr(src)").get()
        item["novel_author"] = response.css("td.info p")[0].css("a::text").get()
        item["novel_category"] = response.css("td.info p")[1].css("a::text").get()
        item["write_status"] = (
            response.css("td.info p")[2].get().split("çŠ¶æ€ï¼š")[-1].split("</p>")[0]
        )
        item["updated_time"] = (
            response.css("td.info p")[3].get().split("æ›´æ–°ï¼š")[-1].split("</p>")[0]
        )
        item["intro"] = (
            response.css("div.intro::text").get().strip()
            if response.css("div.intro::text").get()
            else ""
        )
        
        yield item

        # item["chapter_list"] = []

        # #  è·å–å°è¯´ç›®å½•page
        # mulu_node=response.css("div.lb_mulu div.input-group").get()
        # if not mulu_node:
        #     logging.warning(f"{item["novel_name"]} ä»…ä¸€é¡µç›®å½•:{response.url}")
        #     return
        # mulu_page=int(response.css("div.lb_mulu select.form-control option:last-child::text").get().split("ç¬¬")[-1].split("é¡µ")[0])+1
        # for i in range(1,mulu_page):
        #     url = response.url+f"index_{i}.html"
        #     yield scrapy.Request(url, callback=self.parse_chapter, meta={"item": item})
        
    # è·å–å°è¯´ç« èŠ‚
    def parse_chapter(self, response):
        item = response.meta["item"]
        last9 = response.css("ul.last9").getall()
        if len(last9) !=2:
            logging.warning(f"{item["novel_name"]} å¼‚å¸¸:{response.url}")
            return
        chapter_list=scrapy.Selector(text=last9.pop()).css("li a").getall()
        for chapter in chapter_list:
            chapter_id=re.search(r'href="(.*?).html"',chapter).group(1)
            chapter_name=re.search(r'>(.*?)</a>',chapter).group(1)
            chapter_title=chapter_name[chapter_name.index('.')+1:]
            chapter_order=int(chapter_name.split(".")[0])
            item["chapter_list"].append({
                "chapter_id":chapter_id,
                "chapter_title":chapter_title,
                "chapter_order":chapter_order
                })
        # if is_last:
        # yield item
            # console.log("ğŸš€ ~ chapter_url:", chapter_url)
            # console.log("ğŸš€ ~ chapter_name:", chapter_name)
            # yield scrapy.Request(chapter_url, callback=self.parse_content, meta={"item": item})
        # item["content"] = content
    
    # è·å–å°è¯´å†…å®¹
    def parse_content(self, response):
        item = response.meta["item"]
        content = response.css("div#nr1::text").get()
        console.log("ğŸš€ ~ content:", content)
        # item["content"] = content
        # yield item
